{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "### ***The project aims to take data relating to immigration, and perform ETL such that the data can be further analysed. The process will use airflow, and spark to co-ordinate the retrieval of the data, and transformation into fact and dimension tables. These will be stored in amazon redshift so that the users(data scientists) can query the data efficiently and perform the analytics. To accomplish the purpose of this project first perform  extracting and transforming the data using Apache Spark and loading the data to Amazon S3. then loading the star schema from Amazon S3 to Amazon Redshift using Apache Airflow, and performing data quality check*** ###\n",
    "\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "### I decided to use the data sets provided as part of the 'Udacity Provided Project'. This data consists of data related to immigration in the United States.###\n",
    "\n",
    "The data collected was as follows;\n",
    "\n",
    "* I94 Immigration Data - This data was from the US National Tourism and Trade Office. The data contains international visitor arrival statistics by world regions, and select countries. The data contains the type of visa, the mode of transportation, the age groups, states visited, and the top ports of entry for immigration into the United States.\n",
    "\n",
    "* US Cities: Demographics - This dataset contains information about the demographics of all US cities, and census-designated places with a population greater or equal to 65,000. The dataset can be accessed here \n",
    "\n",
    "### Firstly we will aim to understand the schema of the data collected. The aim of the process will be to develop a data pipeline, such that the provided data can be transformed, cleaned, and loaded into a data warehouse. The aim will be to develop the data warehouse such that relevant insights can be extracted easily. We will have a few outcomes we wish to satisfy in the process ###\n",
    "\n",
    "* The data must be stored in fact, and dimensional tables\n",
    "* The data must be cleaned, such that it can be queryable  \n",
    "* The data must be stored such that database joins can be easily made to correlate data sources \n",
    "\n",
    "# The users wanted to perform the following analytics:\n",
    "* find the state that has the most immigrants\n",
    "* find out the state that has the least immigrants\n",
    "* find out the state that has the most ratio of white people\n",
    "* find out the state that has the least ratio of white people\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration = df_immigration \\\n",
    "        .withColumn(\"cicid\", df_immigration[\"cicid\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"i94yr\", df_immigration[\"i94yr\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"i94mon\", df_immigration[\"i94mon\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"i94cit\", df_immigration[\"i94cit\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"i94res\", df_immigration[\"i94res\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"arrdate\", df_immigration[\"arrdate\"].cast(StringType())) \\\n",
    "        .withColumn(\"i94mode\", df_immigration[\"i94mode\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"depdate\", df_immigration[\"depdate\"].cast(StringType())) \\\n",
    "        .withColumn(\"i94bir\", df_immigration[\"i94bir\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"i94visa\", df_immigration[\"i94visa\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"count\", df_immigration[\"count\"].cast(IntegerType())) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration = df_immigration.select(\"cicid\",\"i94yr\",\"i94mon\",\"i94cit\",\"i94res\",\"i94port\",\"arrdate\",\"i94mode\",\"i94addr\",\"depdate\",\"i94bir\",\"i94visa\",\"count\",\"gender\",\"airline\",\"fltno\",\"visatype\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics = spark.read.format(\"csv\").option(\"delimiter\", \";\").option(\"header\",\"true\").load(\"us-cities-demographics.csv\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics = df_demographics \\\n",
    "        .withColumn(\"Total Population\", df_demographics[\"Total Population\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"Male Population\", df_demographics[\"Male Population\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"Female Population\", df_demographics[\"Female Population\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"Number of Veterans\", df_demographics[\"Number of Veterans\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"Foreign-born\", df_demographics[\"Foreign-born\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"Count\", df_demographics[\"Count\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics = df_demographics \\\n",
    "        .withColumnRenamed(\"City\", \"city\") \\\n",
    "        .withColumnRenamed(\"State\", \"state\") \\\n",
    "        .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "        .withColumnRenamed(\"Male Population\", \"male_population\") \\\n",
    "        .withColumnRenamed(\"Female Population\", \"female_population\") \\\n",
    "        .withColumnRenamed(\"Total Population\", \"total_population\") \\\n",
    "        .withColumnRenamed(\"Number of Veterans\", \"num_of_veterans\") \\\n",
    "        .withColumnRenamed(\"Foreign-born\", \"foreign_born\") \\\n",
    "        .withColumnRenamed(\"Average Household Size\", \"avg_household_size\") \\\n",
    "        .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "        .withColumnRenamed(\"Race\", \"race\") \\\n",
    "        .withColumnRenamed(\"Count\", \"count\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pivot_df_demographics = df_demographics.groupBy(\"state_code\").pivot(\"race\").sum(\"count\").sort(\"state_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pivot_df_demographics = pivot_df_demographics \\\n",
    "        .withColumnRenamed(\"American Indian and Alaska Native\", \"indian\") \\\n",
    "        .withColumnRenamed(\"Asian\", \"asian\") \\\n",
    "        .withColumnRenamed(\"Black or African-American\", \"black\") \\\n",
    "        .withColumnRenamed(\"Hispanic or Latino\", \"hispanic\") \\\n",
    "        .withColumnRenamed(\"White\", \"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pivot_df_demographics.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_list_demographics = []\n",
    "for i in range(pivot_df_demographics.select(\"state_code\").dropDuplicates().count()):\n",
    "    state_list_demographics.append(str(pivot_df_demographics.select(\"state_code\").dropDuplicates().collect()[i][\"state_code\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(state_list_demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_filtered = df_immigration.filter(F.col(\"i94addr\").isin(state_list_demographics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_filtered.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Extracting fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_source_df_immigration = df_immigration_filtered.groupBy(\"i94addr\").agg(F.sum(df_immigration_filtered[\"count\"]).alias(\"immigration_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_source_df_demographics = pivot_df_demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_source_df_demographics.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table = fact_source_df_immigration.join(fact_source_df_demographics, fact_source_df_immigration.i94addr == fact_source_df_demographics.state_code).withColumn(\"fact_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table = fact_table.select(\"fact_id\",\"state_code\",\"immigration_count\",\"white\",\"asian\",\"black\",\"hispanic\",\"indian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table = fact_table \\\n",
    "        .withColumn(\"immigration_count\", fact_table[\"immigration_count\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"white\", fact_table[\"white\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"asian\", fact_table[\"asian\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"black\", fact_table[\"black\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"hispanic\", fact_table[\"hispanic\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"indian\", fact_table[\"indian\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Extracting dimemsion tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### dim_state_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_state_table = df_demographics.groupBy(\"state_code\",\"state\").agg(F.sum(df_demographics[\"foreign_born\"])).select(\"state_code\",\"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_state_table.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## dim_visa_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pivot_df_immigration_visa = df_immigration_filtered.groupBy(\"i94addr\").pivot(\"visatype\").sum(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pivot_df_immigration_visa.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_visa_table = pivot_df_immigration_visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_visa_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_visa_table = dim_visa_table \\\n",
    "        .withColumnRenamed(\"i94addr\", \"state_code\") \\\n",
    "        .withColumn(\"B1\", dim_visa_table[\"B1\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"B2\", dim_visa_table[\"B2\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"CP\", dim_visa_table[\"CP\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"CPL\", dim_visa_table[\"CPL\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"E1\", dim_visa_table[\"E1\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"E2\", dim_visa_table[\"E2\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"F1\", dim_visa_table[\"F1\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"F2\", dim_visa_table[\"F2\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"GMT\", dim_visa_table[\"GMT\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"I\", dim_visa_table[\"I\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"I1\", dim_visa_table[\"I1\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"M1\", dim_visa_table[\"M1\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"M2\", dim_visa_table[\"M2\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"SBP\", dim_visa_table[\"SBP\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"WB\", dim_visa_table[\"WB\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"WT\", dim_visa_table[\"WT\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_visa_table.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## dim_foreign_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_foreign_table = df_demographics.groupBy(\"state_code\").agg(F.sum(df_demographics[\"foreign_born\"]).alias(\"state_foreign_born\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_foreign_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_foreign_table = dim_foreign_table \\\n",
    "        .withColumn(\"state_foreign_born\", dim_foreign_table[\"state_foreign_born\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_foreign_table.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ETL - Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_table.write.mode(\"overwrite\").parquet(\"s3a://my-bucket/fact_table\")\n",
    "dim_state_table.write.mode('overwrite').parquet(\"s3a://my-bucket/dim_state_table\")\n",
    "dim_visa_table.write.mode('overwrite').parquet(\"s3a://my-bucket/dim_visa_table\")\n",
    "dim_foreign_table.write.mode('overwrite').parquet(\"s3a://my-bucket/dim_foreign_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Loading parquet data to S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " ## some Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the state that has the most immigrants\n",
    "fact_pdf = fact_table.select(\"*\").toPandas()\n",
    "fact_pdf[fact_pdf.immigration_count == fact_pdf.immigration_count.max()]\n",
    "str(fact_pdf[fact_pdf.immigration_count == fact_pdf.immigration_count.max()]['state_code'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#state that has the least immigrants\n",
    "fact_pdf[fact_pdf.immigration_count == fact_pdf.immigration_count.min()]\n",
    "str(fact_pdf[fact_pdf.immigration_count == fact_pdf.immigration_count.min()]['state_code'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#state that has the most ratio of white people\n",
    "fact_pdf['white_percentage'] = (fact_pdf['white']/(fact_pdf['white']+fact_pdf['asian']+fact_pdf['black']+fact_pdf['hispanic']+fact_pdf['indian']) * 100).round(2)\n",
    "fact_pdf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_pdf = fact_pdf.drop(columns=['white_percentage[%]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_pdf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_pdf[\"white_percentage\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_pdf[fact_pdf.white_percentage == fact_pdf.white_percentage.max()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_pdf[fact_pdf.white_percentage == fact_pdf.white_percentage.max()]['white']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_pdf[fact_pdf.white_percentage == fact_pdf.white_percentage.max()]['white'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "str(fact_pdf[fact_pdf.white_percentage == fact_pdf.white_percentage.max()]['state_code'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#state that has the least ratio of white people\n",
    "fact_pdf[\"white_percentage\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_pdf_2 = fact_pdf.replace(0, fact_pdf[fact_pdf.white_percentage == fact_pdf.white_percentage.max()]['white'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_pdf_2[\"white_percentage\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_pdf_2[fact_pdf_2.white_percentage == fact_pdf_2.white_percentage.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "str(fact_pdf_2[fact_pdf_2.white_percentage == fact_pdf_2.white_percentage.min()]['state_code'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "####  Data dictionary \n",
    "There are the four tables made by extracting and transforming the data from the source data. The first table is the fact table, and the other three tables are dimension tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### fact_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Columns | Description\n",
    "------------ | -------------\n",
    "fact_id | ID that uniquely identify one record in the Fact Table\n",
    "state_code | Code of the state\n",
    "immigration_count | Count of immigration population by state\n",
    "white | Count of white people by state\n",
    "asian | Count of asian people by state\n",
    "black | Count of black people by state\n",
    "hispanic | Count of hispanic people by state\n",
    "indian | Count of Indian people by state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### dim_state_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Columns | Description\n",
    "------------ | -------------\n",
    "state_code | Code of the state\n",
    "state | Full name of the state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### dim_visa_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Columns | Description\n",
    "------------ | -------------\n",
    "state_code | Code of the state\n",
    "B1 | Count of people that has B1 visa by state\n",
    "B2 | Count of people that has B2 visa by state\n",
    "CP | Count of people that has CP visa by state\n",
    "CPL | Count of people that has CPL visa by state\n",
    "E1 | Count of people that has E1 visa by state\n",
    "E2 | Count of people that has E2 visa by state\n",
    "F1 | Count of people that has F1 visa by state\n",
    "F2 | Count of people that has F2 visa by state\n",
    "GMT | Count of people that has GMT visa by state\n",
    "I | Count of people that has I visa by state\n",
    "I1 | Count of people that has I1 visa by state\n",
    "M1 | Count of people that has M1 visa by state\n",
    "M2 | Count of people that has M2 visa by state\n",
    "SBP | Count of people that has SBP visa by state\n",
    "WB | Count of people that has WB visa by state\n",
    "WT | Count of people that has WT visa by state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### dim_foreign_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Columns | Description\n",
    "------------ | -------------\n",
    "state_code | Code of the state\n",
    "state_foreign_born | Count of people who were foreign born by state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "Tools & Technologies\n",
    "* Python : Python is used as the base programming language to perform the ETL of the data.\n",
    "\n",
    "* Spark : Pyspark is used to process the Big data and load it to Amazon S3.\n",
    "\n",
    "* Amazon S3 : Amazon S3 is used for Storing processed outputs.\n",
    "\n",
    "* Airflow : Airflow is used to load the star schema to Amazon Redshift and to perform data quality check.\n",
    "\n",
    "* Amazon Redshift : Amazon Redshift is used as warehousing database.\n",
    "\n",
    "* Pandas : Pandas is used to perform Data Analytics.\n",
    "### Alternative approach to solve problem under different scenarios\n",
    "* The data was increased by 100x\n",
    "The Redshift Cluster Node type dc2.large can handle storage of 160GB/node and can have a maximum of 32 nodes(5.1 TB Total compressed storage). So Redshift can easily handle the data increased by 100x.\n",
    "\n",
    "* The pipelines would be run on a daily basis by 7 am every day\n",
    "To run the pipelines on a daily basis by 7 am every day, add schedule_interval='0 7 * * *' to the DAG .\n",
    "```python\n",
    "default_args = {\n",
    "    'owner': 'ezzat',\n",
    "    'start_date': datetime.now(),\n",
    "    'depends_on_past': False,\n",
    "    'email_on_retry': False,\n",
    "    'email_on_failure': False,\n",
    "    'catchup': False\n",
    "}\n",
    " dag_name = 'capstone_project_dag' \n",
    "dag = DAG(dag_name,\n",
    "          default_args=default_args,\n",
    "          description='Extract Load and Transform data from S3 to Redshift',\n",
    "          schedule_interval='0 7 * * *'\n",
    "        )\n",
    "```\n",
    "\n",
    "* The database needed to be accessed by 100+ people\n",
    "The maximum number of Redshift connections is 500 and 50 can run in parallel at a point in time, so 100+ people can easily connect to Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## how to run \n",
    "\n",
    "* You have to run all the cells from the CapstoneProjectTemplate.ipynb file in order to create the star-schema tables and perform the ETL process.\n",
    "\n",
    "* To create the tables in the Redshift cluster, run create_tables.py file The command should look like this: python create_tables.py\n",
    "\n",
    "* Finally, turn the capstone_project_dag DAG ON and trigger the DAG on airflow.\n",
    "## hint: i used airflow in data pipeline project in the previous course . i have add three files 1 for testing data (data_quality.py) , 1 for operators (load_to_redshift.py) and 1 for dags (load_to_redshift.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
